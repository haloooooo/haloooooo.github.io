---
layout:     post
title:      "神经网络"
subtitle:   "《机器学习》 chapter 5"
header-img: "img/post-bg-js-version.jpg"
date:       2018-03-28
author:     "Linzb"
tags:
    - 西瓜书《机器学习》
    - machine learning
---
# 神经网络
人工神经网络（artificial neural network，缩写ANN），简称神经网络(NN)，在机器学习领域，是一种模仿生物神经网络的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的神经元联结进行计算。大多数情况下神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具。典型的神经网络具有以下三个部分：

>- 结构（Architecture） 指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量:神经元连接的权重（weights）和神经元的激励值（activities of the neurons）。

>- 激励函数（Activity Rule）大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激励函数依赖于网络中的权重。

>- 学习规则（Learning Rule）学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间尺度的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的目标值和当前权重的值。例如，用于手写识别的一个神经网络，有一组输入神经元。输入神经元会被输入图像的数据所激发。在激励值被加权并通过一个函数（由网络的设计者确定）后，这些神经元的激励值被传递到其他神经元。这个过程不断重复，直到输出神经元被激发。最后，输出神经元的激励值决定了识别出来的是哪个字母。

神经网络的构筑理念是受到生物神经网络功能的运作启发而产生的。和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。
## 零、神经元
1943年，基于生物神经网络的麦卡洛克-皮茨神经元模型（McCulloch－Pitts′ neuron model）诞生。它由心理学家Warren McCulloch和数学家Walter Pitts合作提出。M-P神经元模型，也称“阈值逻辑单元”。

McCulloch-Pitts模型公式如下：

![](/img/in-post/2018-03-28-ML-chapter5-mp.png)

其中阈值b, 亦称bias，下图中把阈值b看作一个输入固定为1.0、链接权重为b的“哑结点”（dummy node)。这样，权重和阈值的学习就可以统一为权重的学习。

神经元示意图：

![](/img/in-post/2018-03-28-ML-chapter5-TLU.png)

```
    a1~an为输入向量的各个分量
    w1~wn为神经元各个突触的权值
    b为偏置
    t为神经元输出

    f为传递函数、激励函数、响应函数，通常为非线性函数。基本作用：
          1、控制输入对输出的激活作用；
          2、对输入、输出进行函数转换；
          3、将可能无限域的输入变换成指定的有限范围内的输出。

```

数学表示  ![](/img/in-post/2018-03-28-ML-chapter5-t.svg)
```
    向量W为权向量,做内积需要用到转置
    向量A为输入向量
    b为偏置（bias），或者称之为阈值（threshold）
    f为传递函数，也即激励函数
```
可见，一个神经元的功能是求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量结果。

单个神经元的作用：把一个n维向量空间用一个超平面分区成两部分（称之为判断边界），给定一个输入向量，神经元可以判断出这个向量位于超平面的哪一边。




##  一、感知机
感知器(Perceptron)是Frank Rosenblatt在1957年就职于Cornell航空实验室(Cornell Aeronautical Laboratory)时所发明的一种人工神经网络。它被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。在人工神经网络领域中，感知机也被指为单层的人工神经网络，以区别于较复杂的多层感知机(Multilayer Perceptron)。

作为一种线性分类器，感知机可说是最简单的前向人工神经网络形式。感知机结构简单，由两层神经元组成，输入层接收外界信号，输出层只有一个神经元，也称为神经网络的一个处理单元(PE，Processing Element)。所以一台感知机只有一个输出，可以有多个输入。如下图：

![](/img/in-post/2018-03-28-ML-chapter5-perceptron.jpg)


### 感知机损失函数

![](/img/in-post/2018-03-28-ML-chapter5-perceptron-loss.jpg)

### 感知机学习算法

感知机学习算法是对感知机损失函数的最优化问题的算法。感知机学习算法是误分类驱动的，先随机选取一个超平面，然后用梯度下降法不断极小化上述损失函数。损失函数的参数(权重)加上梯度上升的反方向，于是就梯度下降了。所以，上述迭代可以使损失函数不断减小，直到为0。于是得到了原始形式的感知机学习算法：

![](/img/in-post/2018-03-28-ML-chapter5-perceptiron-learn.jpg)

感知器是整个神经网络的基础，神经元通过响应函数确定输出，神经元之间通过权值进行传递信息， 权重的确定根据误差来进行调节，这就是学习的过程。这个方法的前提是整个网络是收敛的，1957年罗森布拉特证明了这个结论。

### 线性分类与异或问题

感知器简单而优雅，但它仅对线性问题具有分类能力。什么是线性问题呢？简单来讲，就是用一条直线可分的图形。比如，逻辑“与”和逻辑“或”就是线性问题，我们可以用一条直线来分隔0和1。

逻辑“与”的真值表和二维样本图：

![](/img/in-post/2018-03-28-ML-chapter5-perceptron-and.jpg)

逻辑“或”的真值表和二维样本图：

![](/img/in-post/2018-03-28-ML-chapter5-perceptron-or.jpg)

为什么感知器可以解决线性问题呢？这是由它的权重求和公式决定的。这里以两个输入分量 x1 和 x2 组成的二维空间为例，此时神经元的输出为
```
  o = 1,  ω1 * x1 + ω2 * x2 − T > 0
     -1,  ω1 * x1 + ω2 * x2 − T < 0
```
所以，方程```ω1 * x1 + ω2 * x2 − T = 0```确定的直线就是二维输入样本空间上的一条分界线。

在三维及更高维数的输入样本空间中，```ω1 * x1 + ω2 * x2+...ωn * xn − T = 0```同样表征一个超平面可以划分样本空间。

但是如果要让它来处理非线性的问题，感知器就无能为力了。例如“异或”(XOR，两个输入如果相同，输出为0；两个输入如果是不同，输出为1)，就无法用一条直线来分割开来，因此感知器就没办法实现“异或”的功能。

感知机主要的本质缺陷是它不能处理线性不可分问题。单层的神经网络无法解决上面分析的不可线性分割的问题，典型例子如同或（XNOR，两个输入如果相同，输出为1；两个输入如果是不同，输出为0。）


## 二、异或问题的解决————多层网络

之前我们看到的例子中与、或功能都是线性可分的，感知器只有输出层神经元进行响应函数处理，即只拥有一层功能神经元，学习能力非常有限。因此非线性可分要用到多层神经网络。使用多层的背后思想是：复杂关系功能可以被分解为简单的功能及其组合。

所以异或问题可分解为以下形式，通过多层网络解决。
```
A xnor B = not ( A xor B )
         = not [(A or B) . (A' and B')]
         = (A or B)' or (Aor B')'
         = (A' and B') or (A' and B)  
```

### 1、多层前馈神经网络

多层网络示意图：

![](/img/in-post/2018-03-28-ML-chapter5-net.jpg)

多层神经网络分为三种类型的层：
```
输入层：神经网络最左边的一层，通过这些神经元输入需要训练观察的样本。

隐藏层：介于输入与输出之间的所有节点组成的一层。帮助神经网络学习数据间的复杂关系。

输出层：神经网络最后一层。5个分类的情况下输出层将有5个神经元。
```

![](/img/in-post/2018-03-28-ML-chapter5-net-function.jpg)

由上图可以看出，随着隐层层数的增多，凸域将可以形成任意的形状，因此可以解决任何复杂的分类问题。实际上，Kolmogorov理论指出：双隐层感知器就足以解决任何复杂的分类问题。


### 2、前向传播法

如下图所示，前向传播的思想讲得很清楚了。举个例子，假设上一层结点 i,j,k,… 等一些结点与本层的结点 w 有连接，那么结点 w的 值怎么算呢？就是通过上一层的 i,j,k 等结点以及对应的连接权值进行加权和运算，结果再加上一个偏置项，然后在通过一个非线性函数 f()（即激活函数），如 ReLu，sigmoid 等函数，最后得到的结果就是本层结点 w 的输出。
前向传播就是不断的通过这种方法一层层的运算，在输出层结果。

![](/img/in-post/2018-03-16-PCI-chapter4-2-net2.jpg)

其对应的表达式如下：

![](/img/in-post/2018-03-16-PCI-chapter4-2-net2-detail.jpg)



### 3、反向传播法
多层网络的学习能力比单层感知机的强得多，欲训练多层网络，简单的感知机学习规则显然不够，需要更强大的学习算法，反向传播算法就是其中的杰出代表，它是迄今最成功的神经网络学习算法。

反向传播（Backpropagation, BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。

反向传播要求有对每个输入值想得到的已知输出，来计算损失函数梯度。因此，它通常被认为是一种监督式学习方法。它是多层前馈网络的Delta规则的推广，可以用链式法则对每层迭代计算梯度。反向传播要求神经元（节点）的激励函数可微。

一般而言，只需包含一个足够多神经元的隐层，就能以任意精度逼近任意复杂度的连续函数，故下面以训练单隐层的前馈神经网络为例，介绍BP神经网络的算法思想。

![](/img/in-post/2018-03-28-ML-chapter5-BP.png)

上图为一个单隐层前馈神经网络的拓扑结构，BP神经网络算法也使用梯度下降法（gradient descent），以单个样本的均方误差的负梯度方向对权重进行调节。可以看出：BP算法首先将误差反向传播给隐层神经元，调节隐层到输出层的连接权重与输出层神经元的阈值；接着根据隐含层神经元的均方误差，来调节输入层到隐含层的连接权值与隐含层神经元的阈值。BP算法基本的推导过程与感知机的推导过程原理是相同的，下面给出调整隐含层到输出层的权重调整规则的推导过程：


![](/img/in-post/2018-03-28-ML-chapter5-BP-tuidao.png)

## 三、参考

[各种疑惑、问题都能找到答案：[机器学习] Coursera ML笔记 - 神经网络（Learning）](https://blog.csdn.net/walilk/article/details/50504393)
[统计学习方法笔记：感知机](http://www.hankcs.com/ml/the-perceptron.html)

[异或相关问题：从M-P神经元模型到感知机](https://xueqiu.com/3993902801/83328505)

[感知机-维基百科](https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8)
[人工神经网络-维基百科](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)
[反向传播算法-维基百科](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95)

[技术向：一文读懂卷积神经网络CNN](http://www.cnblogs.com/nsnow/p/4562308.html)

[神经网络中前向传播和反向传播解析](https://blog.csdn.net/lhanchao/article/details/51419150)

[集体智慧编程第四章](https://blog.csdn.net/gavin_yueyi/article/details/49028315)

## 四、问题
损失函数：均方误差？还是[感知机](http://www.hankcs.com/ml/the-perceptron.html)的损失函数 ([感知机-维基百科](https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8)里的准则函数)？

好吧，两个是一样的

感知机的学习算法：[感知机-维基百科](https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8)里的学习算法？ 还是《统计学习方法》[感知机](http://www.hankcs.com/ml/the-perceptron.html)的学习算法

感觉维基的感知机学习算法对应的误差函数是 最小误差平方和
