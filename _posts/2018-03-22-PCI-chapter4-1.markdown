---
layout:     post
title:      "搜索与排名:一个简单的搜索引擎的实现 (一)"
subtitle:   "《集体智慧编程》 chapter 4"
header-img: "img/post-bg-js-version.jpg"
date:       2018-03-22
author:     "Linzb"
tags:
    - 集体智慧编程
    - machine learning
---
# 搜索引擎
一个简单的搜索引擎主要有以下三个部分组成：

    网页抓取：从一个或一组特定的网页开始，根据网页内部链接逐步追踪到其他网页。
             这样递归进行爬取，直到到达一定深度或达到一定数量为止。

    建立索引：建立数据表，包含文档中所有单词的位置信息，文档本身不一定要保存到数据库中，
             索引信息只需简单的保存到一个指向文档所在位置的引用即可。

    查询和排名：根据合适的网页排序方法，返回一个经过排序的页面列表。

## 零、网页抓取：一个简单的爬虫

首先建立一个 Python 模块，取名searchengine，其中包含两个类，本节主要实现 crawler 类，用于检索网页和创建数据库。我们会用到 SQLite 数据库。

程序框架：
```
class crawler:
  # 初始化crawler类并传入数据库名称
  def __init__(self, dbname):
    pass

  def __del__(self):
    pass

  def dbcommit(self):
    pass

  # 辅助函数，用于从table表获取filed元组值为value的条目的id，并且如果条目不存在，就将其加入数据库中
  def getentryid(self, table, field, value, createnew=True):
    return None

  # 为url网页建立索引
  def addtoindex(self, url, soup):
    print('Indexing %s' %url)

  # 从HTML网页中提取文字（不带html标签）
  def gettextonly(self, soup):
    return None

  # 依据任何非字母字符进行分词处理
  def separatewords(self, text):
    return None

  # 如果url已经建过索引，则返回true
  def isindexed(self, url):
    return False

   # 添加一个关联两个网页的链接  #将“原URL—链接URL—链接文本”关系保存到link表
  def addlinkref(self, urlFrom, urlTo, linkText):
    pass

  # 从一小组网页开始进行广度优先搜索，直至某一给定深度，
  # 期间为网页建立索引
  def crawl(self, pages, depth=2):
    pass

  # 创建数据库表
  def createindextables(self):
    pass
```


在此之前，先简单介绍两个相关函数库及 SQLite 数据库。
### 1、urllib库
[urllib](https://docs.python.org/3.6/library/urllib.html)是Python自带的标准库，无需安装，直接可以用。包括以下模块：

    urllib.request 请求模块：     打开和读取URLs
    urllib.error 异常处理模块：    处理由urllib.request引发的异常
    urllib.parse url解析模块
    urllib.robotparser robots.txt解析模块

本章中，将利用 urllib.request 模块下载等待建立索引的网页。

### 2、Beautiful Soup
[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)是一个可以从HTML或XML文件中提取数据的Python库。它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式。

将一段文档传入BeautifulSoup 的构造方法,就能得到一个文档的对象, 可以传入一段字符串或一个文件句柄.

    from bs4 import BeautifulSoup

    soup = BeautifulSoup(open("index.html"))
    soup = BeautifulSoup("<html>data</html>")

几个简单的浏览结构化数据的方法:

    soup.title
    # <title>The Dormouse's story</title>

    soup.title.name
    # u'title'

    soup.title.string
    # u'The Dormouse's story'

    soup.title.parent.name
    # u'head'

    soup.p
    # <p class="title"><b>The Dormouse's story</b></p>

    soup.p['class']
    # u'title'

    soup.a
    # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>

    soup.find_all('a')
    # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
    #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
    #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]

    soup.find(id="link3")
    # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>

### 3、SQL数据库
[SQLite](https://docs.python.org/3.6/library/sqlite3.html?highlight=sqlite#module-sqlite3)是一种嵌入式数据库，它的数据库就是一个文件。由于SQLite本身是C写的，而且体积很小，所以，经常被集成到各种应用程序中，甚至在iOS和Android的App中都可以集成。Python就内置了SQLite3，所以，在Python中使用SQLite，不需要安装任何东西，直接使用。

我们在Python交互式命令行实践一下：
```
# 导入SQLite驱动:
>>> import sqlite3
# 连接到SQLite数据库
# 数据库文件是test.db
# 如果文件不存在，会自动在当前目录创建:
>>> conn = sqlite3.connect('test.db')
# 创建一个Cursor:
>>> cursor = conn.cursor()
# 执行一条SQL语句，创建user表:
>>> cursor.execute('create table user (id varchar(20) primary key, name varchar(20))')
<sqlite3.Cursor object at 0x10f8aa260>
# 继续执行一条SQL语句，插入一条记录:
>>> cursor.execute('insert into user (id, name) values (\'1\', \'Michael\')')
<sqlite3.Cursor object at 0x10f8aa260>
# 通过rowcount获得插入的行数:
>>> cursor.rowcount
1
# 关闭Cursor:
>>> cursor.close()
# 提交事务:
>>> conn.commit()
# 关闭Connection:
>>> conn.close()
```
我们再试试查询记录：
```
>>> conn = sqlite3.connect('test.db')
>>> cursor = conn.cursor()
# 执行查询语句:
>>> cursor.execute('select * from user where id=?', ('1',))
<sqlite3.Cursor object at 0x10f8aa340>
# 获得查询结果集:
>>> values = cursor.fetchall()
>>> values
[(u'1', u'Michael')]
>>> cursor.close()
>>> conn.close()
```
使用Python的DB-API时，只要搞清楚Connection和Cursor对象，打开后一定记得关闭，就可以放心地使用。

使用Cursor对象执行insert，update，delete语句时，执行结果由rowcount返回影响的行数，就可以拿到执行结果。

使用Cursor对象执行select语句时，通过featchall()可以拿到结果集。结果集是一个list，每个元素都是一个tuple，对应一行记录。

如果SQL语句带有参数，那么需要把参数按照位置传递给execute()方法，有几个?占位符就必须对应几个参数，例如：

cursor.execute('select * from user where name=? and pwd=?', ('abc', '123456'))


### 4、爬虫
该爬虫程序首先输入一个初始的网页，获取该网页的内容并进行解析，建立“网页—词—位置”的关系，存入 wordlocation 数据表中。随后从网页中寻找链接，若有链接，则将链接的相对路径转换成绝对路径，并存储，作为下一层检索的URL。同时将“原网页URL—链接URL—链接文本”关系保存到link表中。至此，一层爬取完毕。根据新加入的URL，爬取下一层网页。

```
# 从一小组网页pages开始进行广度优先搜索，直至某一给定深度，
# 期间为网页建立索引
def crawl(self, pages, depth=2):
    for i in range(depth):
        newpages = set()
        for page in pages:
            try:
                c = urllib.request.urlopen(page)  #首先输入一个初始的网页，
            except:
                print("Could not open %s" % page)
                continue
            soup = BeautifulSoup(c.read(), 'html5lib') #获取该网页的内容并进行解析
            self.addtoindex(page, soup)    #建立“网页—词—位置”的关系，存入 wordlocation 数据表中。

            #
            links = soup('a')   
            for link in links:          #从网页中寻找链接
                if ('href' in dict(link.attrs)):
                    url = urljoin(page, link['href']) #若有链接，则将链接的相对路径转换成绝对路径
                    if url.find("'") != -1: continue

                    url = url.split('#')[0]
                    if url[0:4] == 'http' and not self.isindexed(url):
                        newpages.add(url)     #存储，作为下一层检索的URL。
                    linkText = sefl.gettextonly(link)
                    self.addlinkref(page,url,linkText) #将“原URL—链接URL—链接文本”关系保存到link表

            self.dbcommit()
        pages = newpages
```



##  一、建立索引

### 1、建立数据库表

我们通过爬虫程序得到网页的信息之后，我们就需要将这些信息存入数据库，并建立索引。
```
urllist：     保存已经索引过的URL
wordlist：    保存索引到的单词
wordlocation：保存单词在文档中的位置信息（location=i：单词为网页上第i个出现的单词）
link:         保存网页url(fromid)上的超链接url(toid)
linkwords:    保存与对应超链接的相关单词
```

本书中建立的数据库表如下

  ![ ](/img/in-post/2018-03-16-PCI-chapter4-schema.png)


```
def createindextables(self):
    self.con.execute('create table urllist(url)')
    self.con.execute('create table wordlist(word)')
    self.con.execute('create table wordlocation(urlid, wordid, location)')
    self.con.execute('create table link(fromid integer, toid integer)')
    self.con.execute('create table linkwords(wordid, linkid)')

    self.con.execute('create index wordidx on wordlist(word)')
    self.con.execute('create index urlidx on urllist(url)')
    self.con.execute('create index wordurlidx on wordlocation(wordid)')
    self.con.execute('create index urltoidx on link(toid)')
    self.con.execute('create index urlfromidx on link(fromid)')
```
### 2、在网页上查找单词

```
# 从一个HTML网页中提取文字（不带标签的）
def gettextonly(self, soup):
    v = soup.string
    if v == None:
        c = soup.contents
        resulttext = ''
        for t in c:
            subtext = self.gettextonly(t)
            resulttext += subtext +'\n'
        return resulttext
    else:
        return v.strip()

# 根据任何非空白字符进行分词处理
def separatewords(self, text):
    splitter = re.compile('\\W*')
    return [s.lower() for s in splitter.split(text) if s!='']
```

### 3、加入索引


```
# 如果url已经建过索引，则返回true
def isindexed(self, url):
    u = self.con.execute("select rowid from urllist where url=?",(url,)).fetchone()
    if u!= None:
        v = self.con.execute('select * from wordlocation where urlid = ?', (u[0],)).fetchone()
        if v!=None: return True
    return False

# 添加一个关联两个网页的链接
def addlinkref(self, urlFrom, urlTo, linkText):
    words=self.separatewords(linkText)
    fromid=self.getentryid('urllist','url',urlFrom)
    toid=self.getentryid('urllist','url',urlTo)
    if fromid==toid: return
    cur=self.con.execute("insert into link(fromid,toid) values (?,?)", (fromid,toid))
    linkid=cur.lastrowid
    for word in words:
      if word in ignorewords: continue
      wordid=self.getentryid('wordlist','word',word)
      self.con.execute("insert into linkwords(linkid,wordid) values (?,?)", (linkid,wordid))


# 辅助函数，用于获取条目的id，并且如果条目不存在，就将其加入数据库中
def getentryid(self, table, field, value, createnew=True):
    cur = self.con.execute("select rowid from %s where %s='%s'" % (table,field,value))
    res = cur.fetchone()
    if res == None:
        cur = self.con.execute("insert into %s(%s) values (%s)" %(table,field,value))
        return cur.lastrowid
    else:
        return res[0]

# 为每个网页建立索引
def addtoindex(self, url, soup):
    if self.isindexed(url): return
    print('Indexing ' + url)

    text = self.gettextonly(soup)
    words = self.separatewords(text)

    urlid = self.getentryid("urllist", "url", url)

    for i in range(len(words)):
        word = words[i]
        if word in ignorewords: continue
        wordid = self.getentryid('wordlist', 'word', word)
        self.con.execute('insert into wordlocation(urlid, wordid, location) \
                            values(?, ?, ?)', (urlid, wordid, i))
```



## 二、参考

[廖雪峰的官方网站](https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001388320596292f925f46d56ef4c80a1c9d8e47e2d5711000)

[集体智慧编程第四章](https://blog.csdn.net/gavin_yueyi/article/details/49028315)
