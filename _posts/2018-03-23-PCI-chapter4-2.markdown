---
layout:     post
title:      "搜索与排名:一个简单的搜索引擎的实现 (二)"
subtitle:   "《集体智慧编程》 chapter 4"
header-img: "img/post-bg-js-version.jpg"
date:       2018-03-28
author:     "Linzb"
tags:
    - 集体智慧编程
    - machine learning
---
# 排名：对搜索到的网页进行排序
查询和排名：根据合适的网页排序方法，返回一个经过排序的页面列表。

从上一节，我们已经成功获得了与查询条件相关的网页。面对大量网页，我们需要从中找出与查询条件最为匹配的页面。为了解决这个问题，需要一种对网页与查询词相关度的评价方法，并且根据评价结果进行排序。
## 零、基于内容的排序

### 单词频度
~

### 文档位置
~

### 单词距离
~



##  一、利用外部回指指针

### 简单计数
~

### PageRank 算法
~

### 利用链接文本
~


## 二、从点击中学习
在线应用的一个最大优势就在于，它们会持续收到以用户行为为表现形式的反馈信息。对于搜索引擎而言，每一位用户可以通过只点击某条结果，而不选择点击其它内容，向引擎及时提供有关他对搜索结果喜好程度的信息。

为了实现这一目的，我们将构建一个人工神经网络，向其提供：查询条件中的单词，返回给用户的搜索结果，以及用户的点击决策，然后再对其加以训练。一旦网络经过了许多不同的点击训练，我们就可以利用它来改进搜索结果的排序，以更好的反映用户在过去一段时间内的实际点击情况。

### 1、构建神经网络(节点、权重) 存入数据库中
本例中，神经网络结构由三层结构，第一层神经元接受输入，即用户输入的单词；最后一层神经元给予输出，即返回一组URL及其权重列表。而且本例中不是先预先创建一个隐藏层有几千个节点，全部链接均已就绪的巨大网络，我们只在需要时候创建新的隐藏节点会更加高效，更加简单。每传入一组以前从未见过的单词组合，该网络的构建函数就会在隐藏层中建立一个新的节点。随后函数会为单词与隐藏节点之间，以及查询节点与查询所返回的URL结果之间，建立其具有默认权重的链接。随着查询的增多，这个网络也会很大，输入层隐藏层以及输出层之间的链接将会变得非常复杂。
```
from math import tanh
import sqlite3 as sqlite

class searchnet:
    def __init__(self, dbname):
        self.con = sqlite.connect(dbname)

    def __del__(self):
        self.con.close()

    def maketable(self):
        #建立三层神经网络的数据库

        self.con.execute('create table hiddennodes(create_key)')  #隐藏层数据库表
        self.con.execute('create table wordhidden(fromid, toid, strength)') #单词层到隐藏层的权重(连接强度)表
        self.con.execute('create table hiddenurl(fromid, toid, strength)')    #隐藏层到输出层的权重表
        self.con.commit()

    def getstrength(self, fromid, toid, layer):
        #从数据库查询权重(连接强度) 默认值分别为：-0.2 0
        if layer == 0:
            table = 'wordhiden'
        else:
            table = 'hiddenurl'

        res = self.con.execute('select strength from %s where fromid = %d and toid = %d'
                                        % (table, fromid, toid)).fetchone()
        if res == None:
            if layer == 0:
                return -0.2
            if layer == 1:
                return 0
        else:
            return res[0]

    def setstrength(self, fromid, toid, layer, strength):
        #设置（更新）连接强度
        if layer == 0:
            table = 'wordhidden'
        else:
            table = 'hiddenurl'
        res = self.con.execute('select strength from %s where fromid = %d and toid = %d'
                                        %(table, fromid, toid)).fetchone()
        if res == None:
            self.con.execute('insert into %s (fromid, toid, strength) values(%d, %d, %f)'
                                        %(table, fromid, toid, strength))
        else:
            rowid = res[0]
            self.con.execute('update %s set strength = %f where rowid = %d' %(table, strength, rowid))

    def generatehiddennode(self, wordids, urls):
        #输入wordid的list 和对应的url的list 生成隐藏层节点
        if len(wordids) > 3: return None

        #检查我们是否已经为这组单词建立好了一个结点
        createkey = '_'.join(sorted([str(wi) for wi in wordids])) #产生:w1_w2_w3
        res = self.con.execute("select rowid from hiddennodes where create_key ='%s'" %(createkey)).fetchone()

        #如果没有，则建立
        if res == None:
            cur = self.con.execute("insert into hiddennodes(create_key) values('%s')" %(createkey))
            hiddenid = cur.lastrowid
            #设置默认权重
            for wordid in wordids:
                self.setstrength(wordid, hiddenid, 0, 1.0/len(wordids)) #默认权重为1/len(wordids)
            for url in urls:
                self.setstrength(hiddenid, url, 1, 0.1)     #默认权重值为0.1
            self.con.commit()
```

执行以下test代码后：
```
if __name__ == '__main__':
    mynet = searchnet('nn.db')
    #mynet.maketable()
    wWorld, wRiver, wBank = 101, 102, 103
    uWorldBank, uRiver, uEarth = 201,202,203
    mynet.generatehiddennode([wWorld, wBank], [uWorldBank, uRiver, uEarth])
    for c in mynet.con.execute('select * from wordhidden'):
        print(c)
    for c in mynet.con.execute('select * from hiddenurl'):
        print(c)
```
输出：
```
(101, 1, 0.5)
(103, 1, 0.5)
(1, 201, 0.1)
(1, 202, 0.1)
(1, 203, 0.1)
```
创建的神经网络如下图：


![](/img/in-post/2018-03-16-PCI-chapter4-2-example-net.png)

```
x1 = 101    x2 = 103   |   h = 1  | y1 = 201    y2 = 202    y3 = 203
w1 = 0.5    w2 = 0.5   |          | w'1 = 0.1   w'2 = 0.1   w'3 = 0.1
```
### 2、前向传播法
从数据库中取出本次查询将使用到的部分神经网络：
```
def getallhiddenids(self, wordids, urlids):
    #从数据库中取出与当前要查询的wordids、urlids相关联的隐藏层节点
    l1 = {}
    for wordid in wordids:
        cur = self.con.execute('select toid from wordhidden where fromid = %d' % wordid)
        for row in cur: l1[row[0]] = 1
    for urlid in urlids:
        cur = self.con.execute('select fromid from hiddenurl where toid = %d' % urlid)
        for row in cur: l1[row[0]] = 1
    return l1.keys()

def setupnetwork(self, wordids, urlids):
    #建立起与当前要查询的wordids、urlids相关的神经网络
    self.wordids = wordids
    self.hiddenids = self.getallhiddenids(wordids, urlids)
    self.urlids = urlids

    #初始化 节点上的值
    self.ai = [1.0] * len(self.wordids)
    self.ah = [1.0] * len(self.hiddenids)
    self.ao = [1.0] * len(self.urlids)

    #输入层到隐藏层权重
    self.wi = [[self.getstrength(wordid, hiddenid, 0)
                    for hiddenid in self.hiddenids]
                    for wordid in self.wordids]
    #隐藏层到输出层权重
    self.wo = [[self.getstrength(hiddenid, urlid, 1)
                    for urlid in self.urlids]
                    for hiddenid in self.hiddenids]
```
前向传播算法：

如下图所示，前向传播的思想讲得很清楚了。举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，结果再加上一个偏置项，然后在通过一个非线性函数f()（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。
前向传播就是不断的通过这种方法一层层的运算，在输出层结果。

![](/img/in-post/2018-03-16-PCI-chapter4-2-net2.jpg)

其对应的表达式如下：

![](/img/in-post/2018-03-16-PCI-chapter4-2-net2-detail.jpg)

代码如下：
```
def feedforward(self):
    #初始化神经网络的输入
    for i in range(len(self.wordids)):
        self.ai[i] = 1.0

    #计算隐藏层节点的活跃程度
    for j in range(len(self.hiddenids)):
        sum = 0.0
        for i in range(len(self.wordids)):
            sum = sum + self.ai[i] * self.wi[i][j]
        self.ah[j] = tanh(sum)

    #计算输出层节点的活跃程度
    for k in range(len(self.urlids)):
        sum = 0.0
        for j in range(len(self.hiddenids)):
            sum = sum + self.ah[j] * self.wo[j][k]
        self.ao[k] = tanh(sum)
    #返回输出层节点的活跃程度
    return self.ao[:]

def getresult(self, wordids, urlids):
    self.setupnetwork(wordids, urlids)
    return self.feedforward()
```

### 3、反向传播法 训练神经网络
反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。

反向传播要求有对每个输入值想得到的已知输出，来计算损失函数梯度。因此，它通常被认为是一种监督式学习方法，虽然它也用在一些无监督网络（如自动编码器）中。它是多层前馈网络的Delta规则的推广，可以用链式法则对每层迭代计算梯度。反向传播要求人工神经元（或“节点”）的激励函数可微。

代码如下：
```
def backPropagate(self, targets, N = 0.5):
    #计算输出层的误差
    output_deltas = [0.0] * len(self.urlids)
    for k in range(len(self.urlids)):
        error = targets[k] - self.ao[k]
        output_deltas[k] = dtanh(self.ao[k]) * error

    #计算隐藏层误差
    hidden_deltas = [0.0] * len(self.hiddenids)
    for j in range(len(self.hiddenids)):
        error = 0.0
        for k in range(len(self.urlids)):
            error = error + output_deltas[k] * self.wo[j][k]
        hidden_deltas [j] = dtanh(self.ah[j]) * error

    #更新隐藏层-输出层权重
    for j in range(len(self.hiddenids)):
        for k in range(len(self.urlids)):
            change = output_deltas[k] * self.ah[j]
            self.wo[j][k] = self.wo[j][k] + N * change

    #更新输入层-隐藏层权重
    for i in range(len(self.wordids)):
        for j in range(len(self.hiddenids)):
            change = hidden_deltas[j] * self.ai[i]
            self.wi[i][j] = self.wi[i][j] + N * change
```




## 三、参考

[反向传播算法-维基百科](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95)

[技术向：一文读懂卷积神经网络CNN](http://www.cnblogs.com/nsnow/p/4562308.html)

[神经网络中前向传播和反向传播解析](https://blog.csdn.net/lhanchao/article/details/51419150)

[集体智慧编程第四章](https://blog.csdn.net/gavin_yueyi/article/details/49028315)
